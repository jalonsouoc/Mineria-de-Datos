---
title: 'Minería de datos: PEC1'
author: "Autor: Jorge Alonso Hernández"
date: "octubre 2021"
output:
  pdf_document:
    highlight: zenburn
    toc: yes
  word_document: default
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
    includes:
      in_header: 75.584-PEC-header.html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

****
# Introducción
****
## Presentación
Esta prueba de evaluación continuada cubre los módulos "El proceso de minería de datos" y "Preprocesado de los datos y gestión de características" del programa de la asignatura.

## Objetivos
* Asimilar correctamente los módulos citados.
* Qué es y que no es MD.
* Ciclo de vida de los proyectos de MD.
* Diferentes tipologías de MD.
* Conocer las técnicas propias de una fase de conocimiento, preparación de datos y objetivos a lograr.

## Descripción de la PEC a realizar
La prueba está estructurada en 1 ejercicio teórico/práctico y 1 ejercicio práctico que pide que se desarrolle la fase de conocimiento y preparación con un juego de datos.
Se tienen que responderse todos los ejercicios para poder superar la PEC.
La PEC está pensada para resolverla en el entorno Markdown con RStudio con R como lenguaje preferido. Se recomienda hacerlo así. Si tenéis las competencias para hacerlo en Python no hay ningún problema. Podéis hacerlo. Simplemente sustituis los chunks de R por chunks en Python. 

## Recursos
Para realizar esta práctica recomendamos como punto de partida la lectura de los siguientes documentos:

* Los módulos "El proceso de minería de datos" y "Preprocesado de los datos y gestión de características" del programa de la asignatura.
* Ciclo de vida de un proyecto de minería de datos: https://es.wikipedia.org/wiki/cross_industry_standard_process_for_data_mining#Fases_principales
* Al apartado del enunciado de la actividad disponéis de unos materiales de ggplot2
* El aula laboratorio de R para resolver dudas o problemas.
* RStudio Cheat Sheet: Disponible en el aula Laboratorio de Minería de datos.
* R Base Cheat Sheet: Disponible en el aula Laboratorio de Minería de datos.


## Formato y fecha de entrega
El formato de entrega es: **usernameestudiante-PEC1.html (pdf o word) y rmd**. 
Fecha de Entrega: 27/10/2021.
Se tiene que librar la PEC en el buzón de entregas del aula.


## Nota: Propiedad intelectual

A menudo es inevitable, al producir una obra multimedia, hacer uso de recursos creados por terceras personas. Es por lo tanto comprensible hacerlo en el marco de una práctica de los estudios de Informática, Multimedia y Telecomunicación de la UOC, siempre que esto se documente claramente y no suponga plagio en la práctica.

Por lo tanto, al presentar una práctica que haga uso de recursos ajenos, se tiene que presentar junto con ella un documento en que se detallen todos ellos, especificando el nombre de cada recurso, su autor, el lugar donde se obtuvo y su estatus legal: si la obra está protegida por el copyright o se acoge a alguna otra licencia de uso (Creative Commons, licencia GNU, GPL ...).
El estudiante tendrá que asegurarse que la licencia no impide específicamente su uso en el marco de la práctica. En caso de no encontrar la información correspondiente tendrá que asumir que la obra está protegida por copyright.

Habréis, además, adjuntar los ficheros originales cuando las obras utilizadas sean digitales, y su código fuente si corresponde.

****
# Ejemplo de solución mínimo del ejercicio 2
****
****
## Objetivos
****
Como muestra, trabajaremos con el juego de datos "Titanic.csv" que recoge datos sobre el famoso crucero.

Las actividades que llevaremos a cabo en esta práctica se hacen en las fases iniciales de un proyecto de minería de datos. Tienen como objetivo obtener un dominio de los datos con las que construiremos el modelo de minería. Tenemos que conocer profundamente los datos tanto en su formato como contenido. Tareas típicas pueden ser la selección de características o variables, la preparación del juego de datos para posteriormente ser consumido por un algoritmo e intentar extraer el máximo conocimiento posible de los datos. Desarrollaremos un subconjunto de tareas mínimas y de ejemplo. Podemos incluir muchas más y mucho más profundas, como hemos visto en el material docente.



## Procesos iniciales con los datos

Primer contacto con el juego de datos.

Instalamos y cargamos las librerías ggplot2 y dplry.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# https://cran.r-project.org/web/packages/ggplot2/index.html
if (!require('ggplot2')) install.packages('ggplot2'); library('ggplot2')
# https://cran.r-project.org/web/packages/dplyr/index.html
if (!require('dplyr')) install.packages('dplyr'); library('dplyr')
```

Cargamos el fichero de datos.

```{r}
totalData <- read.csv('titanic.csv',stringsAsFactors = FALSE)
filas=dim(totalData)[1]
```

Guardamos los datos filtrados por tripulación para hacer estudios posteriores.

```{r}
totalData_crew=subset(totalData, totalData$class=="engineering crew")
```

Verificamos la estructura del juego de datos principal.

```{r}
str(totalData)
```

Vemos que tenemos 2207 registros que se corresponden a los viajeros y tripulación del Titánic y 11 variables que los caracterizan.

Revisamos la descripción de las variables contenidas al fichero y si los tipos de variable se corresponde al que hemos cargado:

**name**
    string with the name of the passenger.
    
**gender**
    factor with levels male and female.
    
**age**
    numeric value with the persons age on the day of the sinking. The age of babies (under 12 months) is given as a fraction of one year (1/month).
    
**class**
    factor specifying the class for passengers or the type of service aboard for crew members.
    
**embarked**
    factor with the persons place of of embarkment.
    
**country**
    factor with the persons home country.
    
**ticketno**
    numeric value specifying the persons ticket number (NA for crew members).
    
**fare**
    numeric value with the ticket price (NA for crew members, musicians and employees of the shipyard company).
    
**sibsp**
    ordered factor specifying the number if siblings/spouses aboard; adopted from Vanderbild data set.
    
**parch**
    an ordered factor specifying the number of parents/children aboard; adopted from Vanderbild data set.
    
**survived**
    a factor with two levels (no and yes) specifying whether the person has survived the sinking.
    
Vamos ahora a sacar estadísticas básicas y después trabajamos los atributos con valores vacíos.

```{r echo=TRUE, message=FALSE, warning=FALSE}
summary(totalData)
```


Estadísticas de valores vacíos.

```{r}
colSums(is.na(totalData))
colSums(totalData=="")
```

Asignamos valor "Desconocido" para los valores vacíos de la variable "country".

```{r}
totalData$country[is.na(totalData$country)] <- "Desconocido"
```

Asignamos la media para valores vacíos de la variable "age".

```{r}
totalData$age[is.na(totalData$age)] <- mean(totalData$age,na.rm=T)
```


De la información mostrada destacamos que el pasajero más joven tenía 6 meses y el más grande 74 años. La media de edad la tenían en 30 años. También podemos ver 891 sin billete. Revisaremos si se corresponde a la tripulación. También podemos observar el que se pagó por el billete. En este caso se entienden las discrepancias en la fiabilidad de este dato. Parece que los pasajeros que embarcaron a Southampton hacían transbordo de un barco que tenía la tripulación en huelga y por eso no tuvieron que pagar lo que explicaría la diferencia. Recordemos que la tripulación no pagaba. Sibsp y parch también muestran datos interesantes el viajero con quien más familiar viajaba eran 8 hermanos o mujer y 9 hijos o paro/madre.

Si observamos los NA (valores nulos) vemos que los datos están bastante bien. Decidimos sustituir el valor NA de country por Desconocido por una mayor legibilidad. También proponemos sustituir los NA de age por la media a pesar de que realmente no hace falta.

Es curios como los valores NA de sibsp y parch nos permite deducir que viajaban muchas familias. De hecho a simple vista, restante la tripulación la gente que viajaba sola era mínima. Este dato la podríamos contrastar también. Sería interesante relacionar la mortalidad del accidente con el tamaño de las familias que viajaban.

Ahora añadiremos un campo nuevo a los datos. Este campos contendrá el valor de la edad discretitzada con un método simple de intervalos de igual amplitud.

```{r echo=TRUE, message=FALSE, warning=FALSE}

summary(totalData[,"age"])
```

Discretizamos con intervalos.

```{r}
totalData["segmento_edad"] <- cut(totalData$age, breaks = c(0,10,20,30,40,50,60,70,100), labels = c("0-9", "10-19", "20-29", "30-39","40-49","50-59","60-69","70-79"))
```

Observamos los datos discretizados.

```{r}
head(totalData)
```

Vemos como se agrupaban por edad.

```{r}
plot(totalData$segmento_edad,main="Número de pasajeros por grupos de edad",xlab="Edad", ylab="Cantidad",col = "ivory")
```

Ahora repetimos por el proceso pero solo por el subconjunto de tripulación filtrado antes.

```{r}
totalData_crew["segmento_edad"] <- cut(totalData_crew$age, breaks = c(0,10,20,30,40,50,60,70,100), labels = c("0-9", "10-19", "20-29", "30-39","40-49","50-59","60-69","70-79"))
plot(totalData_crew$segmento_edad,main="Número de tripulantes por grupos de edad",xlab="Edad", ylab="Cantidad",col = "ivory")
```

De la discretizaón de la edad observamos que realmente la gente que viajaba era muy joven. El segmento más grande era de 20 a 29 años. También vemos de la juventud de la tripulación.


Como alternativa a la discretización realizada discretizaremos ahora edad con kmeans.

```{r}
# https://cran.r-project.org/web/packages/arules/index.html
if (!require('arules')) install.packages('arules'); library('arules')
set.seed(2)
table(discretize(totalData$age, "cluster" ))
hist(totalData$age, main="Número de pasajeros por grupos de edad con kmeans",xlab="Edad", ylab="Cantidad",col = "ivory")
abline(v=discretize(totalData$age, method="cluster", onlycuts=TRUE),col="red")
```

Podemos observar que sin pasar ningún argumento y que el algoritmo  escoja el conjunto de particiones se muestran tres clústeres que agrupan las edades en las franjas mencionadas.
Podemos asignar el propio clúster como una variable más al dataset para trabajar después.


```{r}
totalData$edad_KM <- (discretize(totalData$age, "cluster" ))
head(totalData)
```

Ahora normalizaremos la edad de los pasajeros por el máximo añadiendo un nuevo valor a los datos que contendrá el valor.

```{r}
totalData$age_NM <- (totalData$age/max(totalData[,"age"]))
head(totalData$age_NM)
```

Supongamos que queremos normalizar por la diferencia para ubicar entre 0 y 1 la variable edad del pasajero dado que el algoritmo de minería que utilizaremos así lo requiere. observamos la distribución de la variable original y las tres generadas

```{r}
totalData$age_ND = (totalData$age-min(totalData$age))/(max(totalData$age)-min(totalData$age))

max(totalData$age)
min(totalData$age)
hist(totalData$age,xlab="Edad", col="ivory",ylab="Cantidad", main="Número de pasajeros por grupos de edad")
hist(totalData$age_NM,xlab="Edad normalizada por el máximo", ylab="Cantidad",col="ivory", main="Número de pasajeros por grupos de edad")
hist(totalData$age_ND,xlab="Edad normalizada por la diferencia",ylab="Cantidad", col="ivory", main="Número de pasajeros por grupos de edad")
```

## Procesos de análisis visuales del juego de datos

Nos proponemos analizar las relaciones entre las diferentes variables del juego de datos para ver si se relacionan y como.

Visualizamos la relación entre las variables "gender" y "survived":

```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(data=totalData[1:filas,],aes(x=gender,fill=survived))+geom_bar()+ggtitle("Relación entre las variables gender y survived")
```


Otro punto de vista. Survived como función de Embarked:

```{r}
ggplot(data=totalData[1:filas,],aes(x=embarked,fill=survived))+geom_bar(position="fill")+ylab("Frequència")+ggtitle("Survived como función de Embarked")
```

En la primera gráfica podemos observar fácilmente la cantidad de mujeres que viajaban respecto hombres y observar los que no sobrevivieron. Numéricamente el número de hombres y mujeres supervivientes es similar.

En la segunda gráfica de forma porcentual observamos los puertos de embarque y los porcentajes de supervivencia en función del puerto. Se podría trabajar el puerto C (Cherburgo) para ver de explicar la diferencia en los datos. Quizás porcentualmente embarcaron más mujeres o niños... ¿O gente de primera clase?

Obtenemos ahora una matriz de porcentajes de frecuencia.
Vemos, por ejemplo que la probabilidad de sobrevivir si se embarcó en "C" es de un 56.45%

```{r echo=TRUE, message=FALSE, warning=FALSE}
t<-table(totalData[1:filas,]$embarked,totalData[1:filas,]$survived)
for (i in 1:dim(t)[1]){
    t[i,]<-t[i,]/sum(t[i,])*100
}
t
```

Veamos ahora como en un mismo gráfico de frecuencias podemos trabajar con 3 variables: Embarked, Survived y class.

Mostramos el gráfico de embarcados por Pclass:

```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(data = totalData[1:filas,],aes(x=embarked,fill=survived))+geom_bar(position="fill")+facet_wrap(~class)+ggtitle("Pasajeros por clase, puerto de origen y relación con survived")
```

Aquí ya podemos extraer mucha información. Como propuesta de mejora se podría hacer un gráfico similar trabajando solo la clase. Habría que unificar toda la tripulación a una única categoría.

Comparamos ahora dos gráficos de frecuencias: Survived-SibSp y Survived-Parch

```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(data = totalData[1:filas,],aes(x=sibsp,fill=survived))+geom_bar()+ggtitle("Sobrevivir en función de tener a bordo cónyuges y/o hermanos")
ggplot(data = totalData[1:filas,],aes(x=parch,fill=survived))+geom_bar()+ggtitle("Sobrevivir en función de tener a bordo padres y/o hijos")
```

Vemos como la forma de estos dos gráficos es similar. Este hecho nos puede indicar presencia de correlaciones altas. Hecho previsible en función de la descripción de las variables.

Veamos un ejemplo de construcción de una variable nueva: Tamaño de familia

```{r echo=TRUE, message=FALSE, warning=FALSE}
totalData$FamilySize <- totalData$sibsp + totalData$parch +1;
totalData1<-totalData[1:filas,]
ggplot(data = totalData1[!is.na(totalData[1:filas,]$FamilySize),],aes(x=FamilySize,fill=survived))+geom_histogram(binwidth =1,position="fill")+ylab("Frecuencia")+ggtitle("Sobrevivir en función del número de familiares a bordo")
```
Se confirma el hecho de que los pasajeros viajaban mayoritariamente en familia. No podemos afirmar que el tamaño de la familia tuviera nada que ver con la posibilidad de sobrevivir pues nos tememos que estadísticamente el hecho de haber más familias de alrededor de cuatro miembros debería de ser habitual. Es un punto de partida para investigar más.

Veamos ahora dos gráficos que nos comparan los atributos Age y Survived.
Observamos como el parámetro position="fill" nos da la proporción acumulada de un atributo dentro de otro

```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(data = totalData1[!(is.na(totalData[1:filas,]$age)),],aes(x=age,fill=survived))+geom_histogram(binwidth =3)+ggtitle("Sobrevivir en función de edad")
ggplot(data = totalData1[!is.na(totalData[1:filas,]$age),],aes(x=age,fill=survived))+geom_histogram(binwidth = 3,position="fill")+ylab("Frecuencia")+ggtitle("Sobrevivir en función de edad")
```


Observamos como el parámetro position="hijo" nos da la proporción acumulada de un atributo dentro de otro. Parece que los niños tuvieron más posibilidad de salvarse.

Vamos a probar si hay una correlación entre la edad del pasajero y el que pagó por el viaje

```{R}
# https://cran.r-project.org/web/packages/tidyverse/index.html
if (!require('tidyverse')) install.packages('tidyverse'); library('tidyverse')
cor.test(x = totalData$age, y = totalData$fare, method = "pearson")
ggplot(data = totalData, aes(x = age, y = log(fare))) + geom_point(color = "gray30") + geom_smooth(color = "firebrick") + theme_bw() +ggtitle("Correlación entre precio billete y edad")
```


Cómo podemos observar no parece haber correlación lineal entre la edad del pasajero y el precio del billete. El diagrama de dispersión tampoco apunta a ningún tipo de relación no lineal evidente.


## Conclusiones finales

Los datos tienen una calidad correcta y están mayoritariamente bien informados. Disponen de una variable de clase "survived" que los hace aptos para un clasificador.
A parte de la mayor supervivencia de mujeres y niños y de pasajeros de primera clase podemos observar la juventud de los pasajeros y la tripulación. Se observa también una gran cantidad de personas que viajaban en familia.

****
# Ejercicios
****

## Ejercicio 1:

Propon un proyecto completo de minería de datos. La organización de la respuesta tiene que coincidir con las fases típicas del ciclo de vida de un proyecto de minería de datos. *No hay que hacer las tareas de cada fase*. Para cada fase indica cuál es el objetivo de la fase y el producto que se obtendrá. Utiliza ejemplos de qué y como podrían ser las tareas. Si hay alguna característica que hace diferente el ciclo de vida de un proyecto de minería respecto a otros proyectos indícalo.


 >Las fases típicas de un proyecto de minería de datos consistirían en: definición de la tarea de minería de datos, selección de datos, preparación de los datos, minería de datos, evaluación e interpretación del modelo e integración.
 
>**Definición de la tarea de minería de datos.**
El primer paso será definir el objetivo de nuestro proyecto de minería de datos, en este caso proponemos un proyecto de predicción. El objetivo de nuestro modelo de datos va a consistir en predecir el grado de éxito o fracaso de una película en la recaudación de la taquilla española para aquellas películas estrenadas en los cines españoles.

>**Selección de datos.**
Los datos para poder realizar el proyeto de minería de datos los obtendremos de la página del ministerio de cultura y deportes. La cual consideramos que tienes los datos más verídicos de la recaudación de la taquilla en los cines españoles.

>**Preparación de los datos.**
Después de obtener los datos del ministerio de cultura y deporte debemos de asegurarnos de que los datos tengan la calidad suficiente, que sean los necesarios y que estén en la forma adecuada.
En primer lugar, realizaremos una primera fase de limpieza en la cual nos encargaremos de eliminar los datos replicados de nuestra muestra, así como de eliminar aquellos datos que no contengan la recaudación en taquilla pues no nos serán útiles para nuestro modelo y por último corregiremos las inconsistencias y los errores de transcripción en los valores de texto libre como pueden ser la empresa productora, los directores de las películas o el género de las películas.
En segundo lugar, haremos una transformación de los datos, la principal transformación que debemos realizar es la de convertir la variable recaudación en función a una serie de rangos que determinaremos y asignaremos los valores "Muy poco exitosa", "Poco exitosa", "Éxito moderado", "Exitosa", "Muy exitosa". Estos valores son los que utilizaremos más adelante para predecir el posible éxito de una nueva película.
Por último, reduciremos la muestra a las películas estrenadas en los últimos 5 años para así obtener un mejor modelo que se ajuste a la actualidad y eliminaremos algunos atributos que no consideremos importantes. Consideraremos como atributos importantes, al género al que pertenece la película, empresa productora, director, actor protagonista, la clasificación por edades, la duración y la nacionalidad de la película. El resto de atributos los eliminaremos de la muestra.

>**Minería de datos.**
Una vez que tenemos los datos con la calidad suficiente debemos encontrar el modelo que más se adecue a nuestro objetivo. Con el conocimiento que poseemos a priori y para el objetivo que hemos establecido utilizaremos un árbol de decisiones. En esta etapa del proyecto partiremos de un árbol de decisiones inicial e iniciaremos un proceso de búsqueda hasta que demos con el modelo con mejor calidad en relación con el modelo inicial. Para la construcción de este modelo y como nos estamos basando en metodología de conocimiento a priori, utilizaremos métodos incrementales para ir obteniendo modelos lo más actualizados posibles a los datos que se van generando con los estrenos de nuevas películas.

>**Evaluación e interpretación del modelo.**
En esta etapa pondremos a prueba el modelo obtenido, para nuestro caso, utilizaremos un subconjunto de películas para comprobar si nuestro modelo es capaz de clasificar correctamente las películas de nuestro subconjunto en el grado de éxito que le corresponde. Si nuestro modelo es capaza de acertar en un 95% consideraremos que la calidad de nuestro modelo es buena y podemos pasar a interpretar el modelo. En la interpretación del modelo podemos obtener datos acerca de la importancia de la edad del público al que va dirigida la película en cuanto a sus ingresos, también podremos ver si es más importante el actor principal o el director para asegurar el éxito de la película, e interpretar si una película americana tiene más éxito que una nacional, entre otras cosas.

>**Integración.**
En este último paso integraremos los resultados de la minería de datos. Estos resultados pueden ser utilizados por cines pequeños los cuales dispongan de una serie de salas limitadas y nuestro modelo les va a ayudar a elegir con mayor seguridad las películas a estrenar en sus salas minimizando el riesgo y asegurando la mayor recaudación posible.

## Ejercicio 2:
A partir del juego de datos disponible en el siguiente enlace https://www.kaggle.com/rdoume/beerreviews , realiza las tareas previas a la generación de un modelo de minería de datos explicadas en los módulos "El proceso de minería de datos" y "Preprocesado de los datos y gestión de características". Puedes utilizar de referencia el ejemplo del Titánic.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Cargamos el juego de datos
beerData <- read.csv('beer_reviews.csv',stringsAsFactors = FALSE)


# Redacta aquí el código R para el estudio del juego de datos
#Verificamos la estructura de datos
str(beerData)

```

Comprobamos que tenemos 1586614 registros que corresponden con las reseñas de usuarios a algunos tipos de cerveza y 13 variables que las caracterizan.

Las variables que compondrán cada una de las reseñas contenidas en el fichero corresponden a:

* **brewery_id:** identificador de la cervecería
* **brewery_name:** nombre de la cervecería
* **review_time:** tiempo de la cerveza
* **review_overall:** reseña general
* **review_aroma:** reseña del aroma
* **review_appearance:** reseña de la apariencia
* **review_profilename:** nombre de usuario que realiza la reseña
* **beer_style:** estilo de cerveza 
* **review_palate:** reseña de paladar
* **review_taste:** reseña del sabor
* **beer_name:** nombre de la cerveza
* **beer_abv:** contenido de alcohol de la cerveza
* **beer_beerid:** identificador de la cerveza

Vemos las estadísticas de los datos cargados:

````{r echo=TRUE, message=FALSE, warning=FALSE}
summary(beerData)
head(beerData$review_time)

````

Comprobamos la existencia de valores vacíos:
````{r echo=TRUE, message=FALSE, warning=FALSE}
colSums(is.na(beerData))
colSums(beerData=="")
````

Observamos que para las variables beer_abv, beer_name y review_profile existen valores vacíos, por lo tanto, le asignaremos valores. Para la variable beer_abv le asignaremos la media con precisión de 1 dígito decimal y para la variable beer_name le asignaremos el valor "Unknown".

````{r echo=TRUE, message=FALSE, warning=FALSE}
beerData$beer_abv[is.na(beerData$beer_abv)] <- round(mean(beerData$beer_abv,na.rm=T), digits = 1)
beerData$beer_name[beerData$beer_name==""] <- "Unknown"
beerData$review_profilename[beerData$review_profilename==""] <- "Unknown"
````

De los datos mostrados podemos obtener que tenemos tanto las cervecerías como las cervezas bien identificadas debido a la inexistencia de valores vacíos para las variables de los identificadores de estos campos.

Podemos destacar el máximo grado de alcohol registrado que sería del 57.70% lo cual es un grado de alcohol bastante alto para una cerveza. 

Por último, destacar que para las valoraciones de aroma, paladar y gusto no tenemos una valoración mínima de 0 como ocurre en las otras valoraciones.

Ahora añadiremos una serie de campos nuevos a los datos, los cuales van a corresponder a los valores de las reseñas, pero discretizado en una serie de intervalos, en este caso obtendremos 3 categorías que corresponderían a si la reseña es negativa, positiva o neutra. También añadiremos un campo nuevo para obtener el grado de alcohol en un determinado rango.

````{r echo=TRUE, message=FALSE, warning=FALSE}

beerData["overall_puntuation"] <- cut(beerData$review_overall, breaks = c(-0.01,2.9,3.1,5), labels = c("Negative", "Neutral", "Positive"), right = TRUE)

beerData["aroma_puntuation"] <- cut(beerData$review_aroma, breaks = c(-0.01,2.9,3.1,5), labels = c("Negative", "Neutral", "Positive"), right = TRUE)


beerData["appearance_puntuation"] <- cut(beerData$review_appearance, breaks = c(-0.01,2.9,3.1,5), labels = c("Negative", "Neutral", "Positive"), right = TRUE)

beerData["palate_puntuation"] <- cut(beerData$review_palate, breaks = c(-0.01,2.9,3.1,5), labels = c("Negative", "Neutral", "Positive"), right = TRUE)

beerData["taste_puntuation"] <- cut(beerData$review_taste, breaks = c(-0.01,2.9,3.1,5), labels = c("Negative", "Neutral", "Positive"), right = TRUE)

beerData["abv_grade"] <- cut(beerData$review_taste, breaks = c(0,2,4,6,8,10,60), labels = c("0-2", "2-4", "4-6", "6-8","8-10",">10"))

colSums(is.na(beerData))
colSums(beerData=="")
````
Observamos una  muestra de los datos discretizados

````{r echo=TRUE, message=FALSE, warning=FALSE}
head(beerData)
````

Obtenemos un subconjunto con los datos refrentes a la cervecería denominada Dogfish Head Brewery:
````{r echo=TRUE, message=FALSE, warning=FALSE}
beerDataRewery=subset(beerData, beerData$brewery_id==10099)
head(beerDataRewery)

````

Mostramos como se agrupan en estas categorías las reseñas generales

````{r echo=TRUE, message=FALSE, warning=FALSE}
plot(beerData$overall_puntuation, main="Número de valoraciones generales de cada tipo", xlab="Puntuación", ylab="Cantidad", type="h", col="ivory")
````

Podemos observar que la gran mayoría de puntuaciones son positivas viendo una notable diferencia con el número de valoraciones neutras y negativas.


Nos proponemos ahora a analizar las diferentes reseñas con la reseña general para ver su relación:

Visualizamos la relación entre las puntuaciones generales y las puntuaciones del aroma:

````{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(data=beerDataRewery[1:filas,], aes(x=overall_puntuation, fill=aroma_puntuation))+geom_bar()+ggtitle("Relación entre la puntuación del aroma y la puntuación general")

````

Podemos observar que hay una gran coincidencia entre las valoraciones positivas del aroma y las generales, sin embargo, podemos ver que un porcentaje bastante alto de valoraciones neutras y negativas tienen la valoración del aroma como positiva, lo cual puede indicar que el aroma no es uno de los factores distintivos a la hora de obtener la valoración general.

Visualizamos la relación entre la puntuación general y la puntuación de la apariencia:
````{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(data=beerData[1:filas,], aes(x=overall_puntuation, fill=appearance_puntuation))+geom_bar()+ggtitle("Relación entre la puntuación de la apariencia y la puntuación general")

````

En este caso podemos ver que nuevamente la gran mayoría de valoraciones del aroma positivas coinciden con las valoraciones positivas generales. También podemos ver que coinciden en un alto porcentaje las valoraciones negativas de ambas reseñas. Como contra parte encontramos que las valoraciones de la apariencia neutrales están bastante repartidas por cada una de las valoraciones generales.

Visualizamos la relación entre la puntuación del paladar y la puntuación general:
````{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(data=beerData[1:filas,], aes(x=overall_puntuation, fill=palate_puntuation))+geom_bar()+ggtitle("Relación entre la puntuación de la apariencia y la puntuación general")

````

Visualizamos la relación entre la puntuación del sabor y la puntuación general
````{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(data=beerData[1:filas,], aes(x=overall_puntuation, fill=taste_puntuation))+geom_bar()+ggtitle("Relación entre la puntuación de la apariencia y la puntuación general")

````


Podemos comprobar que tanto la puntuación del paladar y la del sabor son las que más coincidencias tienen con cada una de las reseñas generales, lo cual las distingue como factores relevantes a la hora de puntuar una cerveza.

Visualizamos la relación entre el grado de alcohol y la puntuación general:
`````{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(data=beerData[1:filas,], aes(x=overall_puntuation, fill=abv_grade))+geom_bar()+ggtitle("Relación entre la puntuación general y el grado de alcohol")
````

En este caso podemos concluir en que el grado de alcohol afecta a la valoración general positiva ya que las cervezas de mayor grado sólo están valoradas con puntuaciones positivas, mientras que la gran mayoría de cervezas con un grado de alcohol bajo están valoradas como negativamente. Lo que nos conlleva a deducir que las cervezas que tienen un porcentaje de alcohol entre el 4 y el 6 serán valoradas positivamente.

Analizamos ahora las puntuaciones generales referentes a las diferentes cervezas producidas por la cervercería denominada Dogfish Head Brewery: 

````{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(data = beerDataRewery[1:filas,],aes(x=overall_puntuation))+geom_bar(fill="blue")+facet_wrap(~beer_name)+ggtitle("Cervezas elaboradas por la cervecería Dogfish Head Brewery ")
````

Podemos observar que hay una cerveza que posee una gran número de valoraciones, la cerveza Chicory Stout, lo cual indica que es la más famosa de esta cervecería y la más consumida, podemos ver también que la gran mayoría de reseñas son positivas. Podemos destacar de esta muestra también que el resto de cervezas no gozan de tanto éxito en comparación con la Chicory Stout, si es cierto que hay al menos dos cervezas con bastantes reseñas positivas que serían la Saison Du BUFF y la Johnson's Hellhound.

Analizamos las puntuaciones generales refrentes a los diferentes estilos de cerveza producidas por la cervecería Dogfish Head Brewery: 

````{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(data = beerDataRewery[1:filas,],aes(x=overall_puntuation))+geom_bar(fill="green")+facet_wrap(~beer_style)+ggtitle("Estilos de las cervezas elaboradas por la cervecería Dogfish Head Brewery")
````

En este caso también observamos que existe un estilo de cerveza predominante que sería el American Stout, el cual alberga una gran diferencia de puntuaciones en relación al resto de estilos. También alberga la gran mayoría de puntuaciones positivas por lo que en lo que refiere a esta cervecería es el estilo que más gusta a sus usuarios. podemos destacar que hay ciertos estilos que no gozan de popularidad entre los usuarios como por ejemplo el American Black Ale o Lambic-Fruit, no porque contenga reseñas negativas, sin o debido a la carencia de ellas, lo cual puede indicar que las cervezas con estos estilos son desconocidas para los usuarios.


Podemos concluir del análisis realizado, que los bebedores de cerveza valoran por encima de todo el paladar y el gusto de la cerveza en detrimento de su apariencia y su aroma. también concluimos con que el grado de alcohol parece ser un factor muy importante a la hora de valorar una cerveza.


*** 
# Criterios de evaluación
***

Ejercicio 1

Concepto y peso en la nota final

El objetivo del proyecto está correctamente definido con suficiente concreción y se puede resolver con técnicas de minería de datos. 15%

Las fases del ciclo de vida están bien expresadas. Los ejemplos son clarificadores. Se justifica y argumenta de las decisiones que se han tomado. 20%

Ejercicio 2

Se carga la base de datos, se visualiza su estructura y se explican los hechos básicos de los datos. 5%

Se estudia si existen atributos vacíos o en diferentes escalas que haya que normalizar. Si es el caso se adoptan medidas para tratar estos atributos. Se construye un nueva variable útil a partir de las existentes. Se discretiza algún atributo. 20%
 
Se analizan los datos de forma visual y extraen conclusiones tangibles. Hay que elaborar un discurso coherente y con conclusiones claras. 30%

Se trata en profundidad alguno otro aspecto respecto a los datos presentado en los módulos "Preprocesado de los datos y gestión de características" 10%
